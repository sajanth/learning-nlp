{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d53d88b-72ea-4834-9428-2b5f589f6f6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##Â Interlude: What is a model anyway? <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0525e7a-5881-4f2e-806e-c89442d9ebe9",
   "metadata": {},
   "source": [
    "In order to compare different models, we need to find a way to objectively quantify them. One approach would be to try to measure their \"quality\". By quality, we mean how well a model can \"explain\" a given dataset. A model, in its broadest sence, is  a mathematical function that captures the statistics within a given dataset*. To make this statement more precise, we can consider the space of all possible candidate functions and define a (conditional) probability function over this space as\n",
    "\n",
    "$$\n",
    "p(\\text{model}|\\text{data})\n",
    "$$\n",
    "\n",
    "where $\\text{data}$ is the dataset we are trying to model. Maximizing this probability, as a function of models, is equivalent to finding the best fit for the data. Unfortunately, this probability distribution is usually not directly accessible. Luckily, we can use [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) to rewrite this expression as\n",
    "\n",
    "$$\n",
    "p(\\text{model}|\\text{data}) = \\frac{p(\\text{data}|\\text{model})p(\\text{model})}{p(\\text{data})}\n",
    "$$\n",
    " \n",
    "As it is [often done](https://stats.stackexchange.com/questions/85465/theoretically-why-do-we-not-need-to-compute-a-marginal-distribution-constant-fo), let's ignore the denomiotor and focus on $p(\\text{model})$ (the *prior*) and $p(\\text{data}|\\text{model})$ (the *likelihood*). The prior in some sense captures our knowledge about the data before we look at it. In deep learning tasks, this often manifests through the model architecture choice. Let's say that we are trying to model images and we choose to use convolutional networks. Then, $p(\\text{model})$ is zero for every model that is not a convolutional neural network (or can not be described by one). However, within convolutional neural networks, we still need to find the optimal weights and model parameters. If we have no prior knowledge about these parameters, we can assume that their values are all equally likely**. In this case, $p(\\text{model})$ would be a uniform distribution over the space of all possible convolutional neural networks. This is all just to say that its reasonable to consider that $p(\\text{model}|\\text{data})$ is just proportional to the likelihood\n",
    "\n",
    "$$\n",
    "p(\\text{model}|\\text{data}) \\propto p(\\text{data}|\\text{model})\n",
    "$$\n",
    "\n",
    "So we are able to maximize the left hand side (which we don't have direct access to) by maximizing the likelihood. But note that the likelihood, i.e. \"given a specific model, what is the probability for \\text{data}\", is something we already computed! \n",
    "For example,\n",
    "$$\n",
    "p(\\text{\"Aargauerstrasse\"}|\\text{model}) = p(\"$\",\"A\") * p(\"A\",\"a\") * p(\"a\",\"r\") * ... p(\"e\",\"$\")\n",
    "$$\n",
    "where the terms on the right hand side are the entries of our lookup table!\n",
    "If we compute this we get \n",
    "\n",
    "*Todo: note on overfitting <-> if we make log loss lower and lower we are overfitting because we are maximishing the likelihood for the training data and not the true global distribution.\n",
    "**in practice this is not quite true but lets just roll with the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7ed0e-546a-4d98-a0c9-101e0d19b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [street.rstrip() for street in open(\"../data/streets_zh.txt\")] #.rstrip() removes new line character '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e6e9ba6-aeb7-4df4-ae50-e6db87878641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p($, A) =  0.055776892430278883\n",
      "p(A, a) =  0.00847457627118644\n",
      "p(a, r) =  0.04895104895104895\n",
      "p(r, g) =  0.05328376703841388\n",
      "p(g, a) =  0.15490375802016498\n",
      "p(a, u) =  0.047639860139860137\n",
      "p(u, e) =  0.05530973451327434\n",
      "p(e, r) =  0.16194644696189495\n",
      "p(r, s) =  0.0912845931433292\n",
      "p(s, t) =  0.28368964688926257\n",
      "p(t, r) =  0.5757261410788381\n",
      "p(r, a) =  0.49111937216026436\n",
      "p(a, s) =  0.5721153846153846\n",
      "p(s, s) =  0.3264472736007687\n",
      "p(s, e) =  0.3281287533029066\n",
      "p(e, $) =  0.3303295571575695\n",
      "Likelihood for 'Aargauerstrasse':  1.2080002980894415e-14\n"
     ]
    }
   ],
   "source": [
    "likelihood = 1\n",
    "for pair in zip(names[0], names[0][1:]):\n",
    "        print(f\"p({pair[0]}, {pair[1]}) = \", freq_per_char[pair[0]][pair[1]])\n",
    "        likelihood *=  freq_per_char[pair[0]][pair[1]]\n",
    "print(\"Likelihood for 'Aargauerstrasse': \", likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46656c9e-2e08-495a-940d-6b56f2df99d2",
   "metadata": {},
   "source": [
    "As we are multiplying a lot of probabilities together our total likelihood per word will be very small, which can cause numerical issues. To handle this let us not look at the likelihood but instead the logarithm of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83b7c950-626e-4b67-b99a-d8dd8e12afce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'likelihood' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fm/b7zttxvj0g113b2zbcmnzhgr0000gn/T/ipykernel_24482/4210558539.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'likelihood' is not defined"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "log(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fa5e1-da44-40ff-b12b-2a567a4fd085",
   "metadata": {},
   "source": [
    "The logarithm is a monotinic function, which means that when we maximize the log-likelihood we are also maximizing the likelihood. In the next section we are going to use optimization methods. For historical reasons optimization methods are cast in the language of *minimizing* functions, so lets decide to use the *negative log-likelihood* as our metric for model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77232af9-ea1b-4b74-811b-6b03fbb5a484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.04722495564126"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-log(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453373c-6c28-4a3a-899d-874526a8aa7f",
   "metadata": {},
   "source": [
    "To simply things further, let us not look at the average negative log-likelihood per word (as lenghts of the street names varrry) but per bigram. For the full dataset we arrive at then at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669e45e-83d9-4d01-a32e-39623150f7a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freq_per_char' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fm/b7zttxvj0g113b2zbcmnzhgr0000gn/T/ipykernel_24482/436549236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstreet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0;34m-\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq_per_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'freq_per_char' is not defined"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "n_pairs = sum(count_pairs.values())\n",
    "for street in names:\n",
    "    for pair in zip(street, street[1:]):\n",
    "        total +=  -log(freq_per_char[pair[0]][pair[1]])\n",
    "print(total/n_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
