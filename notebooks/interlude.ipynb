{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd9f929-2317-429b-a949-f7451853a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r freq_per_char names count_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53d88b-72ea-4834-9428-2b5f589f6f6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##Â Interlude: What is a model anyway? <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0525e7a-5881-4f2e-806e-c89442d9ebe9",
   "metadata": {},
   "source": [
    "In order to compare different models we need to find a way to objectively quantify them. One approach would be to try to measure their \"quality\". By quality, we mean how well a model can \"explain\" a given dataset. A model, in its broadest sense, is  a mathematical function that captures the statistics within a given dataset[^1]. To make this statement more precise, we can consider the space of all possible candidate functions and define a (conditional) probability function over this space as\n",
    "\n",
    "$$\n",
    "p(\\text{model}|\\text{data})\n",
    "$$\n",
    "\n",
    "where $\\text{data}$ is the dataset we are trying to model. Maximizing this probability, as a function of models, is equivalent to finding the best fit for the data. Unfortunately, this probability distribution is usually not directly accessible. Luckily, we can use [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) to rewrite this expression as\n",
    "\n",
    "$$\n",
    "p(\\text{model}|\\text{data}) = \\frac{p(\\text{data}|\\text{model})p(\\text{model})}{p(\\text{data})}\n",
    "$$\n",
    " \n",
    "As it is [often done](https://stats.stackexchange.com/questions/85465/theoretically-why-do-we-not-need-to-compute-a-marginal-distribution-constant-fo), let's ignore the denomiotor and focus on $p(\\text{model})$ (the *prior*) and $p(\\text{data}|\\text{model})$ (the *likelihood*). The prior in some sense captures our knowledge about the data before we look at it. In deep learning tasks, this often manifests through the model architecture choice. Let's say that we are trying to model images and we choose to use convolutional networks. Then, $p(\\text{model})$ is zero for every model that is not a convolutional neural network (or can not be described by one). However, within convolutional neural networks, we still need to find the optimal weights and model parameters. If we have no prior knowledge about these parameters, we can assume that their values are all equally likely. In this case, $p(\\text{model})$ would be a uniform distribution over the space of all possible convolutional neural networks. This is all just to say that it's reasonable to consider that $p(\\text{model}|\\text{data})$ is just proportional to the likelihood\n",
    "\n",
    "$$\n",
    "p(\\text{model}|\\text{data}) \\propto p(\\text{data}|\\text{model})\n",
    "$$\n",
    "\n",
    "So we are able to maximize $p(\\text{model}|\\text{data})$ by maximizing the likelihood. But note that the likelihood, i.e. \"given a specific model, what is the probability for $\\text{data}$\", is something we already computed!  \n",
    "For example,\n",
    "$$\n",
    "p(\\text{\"Aargauerstrasse\"}|\\text{model}) = p(\"$\",\"A\") * p(\"A\",\"a\") * p(\"a\",\"r\") * ... p(\"e\",\"$\")\n",
    "$$\n",
    "where the terms on the right hand side are the entries of our lookup table!\n",
    "If we compute this we get \n",
    "\n",
    "[^1]: In theory we are not interested in the model which explains a given particular dataset but rather a model which explains the distribution from which the dataset is a sample from.\n",
    "*Todo: note on overfitting <-> if we make log loss lower and lower we are overfitting because we are maximishing the likelihood for the training data and not the true global distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6e9ba6-aeb7-4df4-ae50-e6db87878641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p($, A) =  0.055776892430278883\n",
      "p(A, a) =  0.00847457627118644\n",
      "p(a, r) =  0.04895104895104895\n",
      "p(r, g) =  0.05328376703841388\n",
      "p(g, a) =  0.15490375802016498\n",
      "p(a, u) =  0.047639860139860137\n",
      "p(u, e) =  0.05530973451327434\n",
      "p(e, r) =  0.16194644696189495\n",
      "p(r, s) =  0.0912845931433292\n",
      "p(s, t) =  0.28368964688926257\n",
      "p(t, r) =  0.5757261410788381\n",
      "p(r, a) =  0.49111937216026436\n",
      "p(a, s) =  0.5721153846153846\n",
      "p(s, s) =  0.3264472736007687\n",
      "p(s, e) =  0.3281287533029066\n",
      "p(e, $) =  0.3303295571575695\n",
      "Likelihood for 'Aargauerstrasse':  1.2080002980894415e-14\n"
     ]
    }
   ],
   "source": [
    "likelihood = 1\n",
    "for pair in zip(names[0], names[0][1:]):\n",
    "        print(f\"p({pair[0]}, {pair[1]}) = \", freq_per_char[pair[0]][pair[1]])\n",
    "        likelihood *=  freq_per_char[pair[0]][pair[1]]\n",
    "print(\"Likelihood for 'Aargauerstrasse': \", likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46656c9e-2e08-495a-940d-6b56f2df99d2",
   "metadata": {},
   "source": [
    "As we are multiplying a lot of probabilities together our total likelihood per word will be very small, which can cause numerical issues. To handle this let us not look at the likelihood but instead at the logarithm of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b7c950-626e-4b67-b99a-d8dd8e12afce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-32.04722495564126"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "log(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fa5e1-da44-40ff-b12b-2a567a4fd085",
   "metadata": {},
   "source": [
    "The logarithm is a monotonic function, which means that when we maximize the log-likelihood we are also maximizing the likelihood. In the next section we are going to use optimization methods. For historical reasons optimization methods are cast in the language of *minimizing* functions, so lets decide to use the *negative log-likelihood* as our metric for model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77232af9-ea1b-4b74-811b-6b03fbb5a484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.04722495564126"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-log(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453373c-6c28-4a3a-899d-874526a8aa7f",
   "metadata": {},
   "source": [
    "To simply things further, let us not look at the average negative log-likelihood per word (as lengths of the street names vary) but per bigram. For the full dataset we arrive then at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3669e45e-83d9-4d01-a32e-39623150f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0474445195968944\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "n_pairs = sum(count_pairs.values())\n",
    "for street in names:\n",
    "    for pair in zip(street, street[1:]):\n",
    "        total +=  -log(freq_per_char[pair[0]][pair[1]]) #log(a*b) = log(a) + log(b)\n",
    "print(total/n_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
